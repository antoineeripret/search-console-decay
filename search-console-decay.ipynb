{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxBRBqsY-PQz",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "See https://github.com/antoineeripret/search-console-decay for the full explanation on how to use this Notebook. Should you have any issue using it, please raise an issue directly in the GitHub repository. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO0Mt75y-FvC",
        "colab_type": "text"
      },
      "source": [
        "## Project Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20bvNi2XlnRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Install or load https://github.com/joshcarty/google-searchconsole. Shoutout to @joshcarty for this amazing library !!  \n",
        "try:\n",
        "  import searchconsole\n",
        "except:\n",
        "  !pip install git+https://github.com/joshcarty/google-searchconsole\n",
        "  import searchconsole\n",
        "\n",
        "#Load other standard libraries that we will use in this notebook. \n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "import calendar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxD-mI8MmB4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Indicate here the property you will use. If you are not sure which ones you can use, just run the folliwng line and pick one. \n",
        "#webproperties = [element for element in webproperties if account[element].permission != 'siteUnverifiedUser']\n",
        "\n",
        "selected_property = 'https://www.liligo.fr/'\n",
        "\n",
        "#We do not want to analyze partial months, hence enter here teh last day of the last month. If we are the 4th of July, we should have \n",
        "# '2020-06-30' here. Format is YYYY-MM-DD. \n",
        "\n",
        "last_day_of_last_month= '2020-04-30'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmQlb0gnmG11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create the account connection. Please note that: \n",
        "# 1. The flow argument is mandatory to use Josh Carty's library in Google Colab \n",
        "# 2. You will have to connect just once with this try / except structure. See https://github.com/joshcarty/google-searchconsole for the explanation.  \n",
        "try:\n",
        "\taccount = searchconsole.authenticate(flow=\"console\",client_config='/content/drive/My Drive/Colab Notebooks/search-console/config/client-secret.json',credentials='/content/drive/My Drive/Colab Notebooks/search-console/config/credentials.json')\n",
        "\n",
        "except:\n",
        "\taccount = searchconsole.authenticate(flow=\"console\",client_config='/content/drive/My Drive/Colab Notebooks/search-console/config/client-secret.json',serialize='/content/drive/My Drive/Colab Notebooks/search-console/config/credentials.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DQFEYHAnB13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Retrieve the top content from your GSC data. You can add more filters if you have to. \n",
        "# Based on Pareto rule, you shouldn't need to retrieve more than 100 contents for your first analysis. \n",
        "# Moreover, retrieving 100 URLs could be quite long using the API ;) \n",
        "\n",
        "#List of URLs we just mentioned \n",
        "urls = account[selected_property].query.filter('page','magazine','contains').range('today', days=-1000).dimension('page').limit(25).get().to_dataframe()['page']\n",
        "\n",
        "#We initiate an empty DataFrame we will use later on \n",
        "merged_gsc_data = pd.DataFrame()\n",
        "\n",
        "#Function to normalize months: if the month number has just one digit, we add a 0 at the beginning \n",
        "def normalizemonth(x):\n",
        "\tif len(str(x))==1:\n",
        "\t\treturn '0'+str(x)\n",
        "\telse:\n",
        "\t\treturn str(x) \n",
        "\n",
        "#We start a loop\n",
        "i=0\n",
        "#We loop our urls list that is storing our contents urls \n",
        "for url in urls[0:5]:\n",
        "  #We print i just to follow the advance of our cell inside Colab \n",
        "  print(i)\n",
        "  # We retrieve all data available until the last day of last month. \n",
        "  # We add -1000 as the days arguments to be sure to retrieve all data available. \n",
        "  gsc_data = account[selected_property].query.filter('page',url,'equals').range(last_day_of_last_month,days=-1000).dimension('page','date').get().to_dataframe()\n",
        "\t#Convert date to datetime object \n",
        "  gsc_data['date'] = pd.to_datetime(gsc_data['date'],format='%Y-%m-%d')\n",
        "  #Extract month and normalize it using our custom function \n",
        "  gsc_data['month'] = gsc_data['date'].dt.month.apply(normalizemonth)\n",
        "  #We build a yearMonth column  \n",
        "  gsc_data['yearMonth'] = gsc_data['date'].dt.year.astype(str)+gsc_data['month']\n",
        "  #We group data per yearMonth to analyze data monthly and not daily, which is better  \n",
        "  gsc_data = gsc_data.groupby(['yearMonth']).sum()\n",
        "  #Remove useless columns and reset index \n",
        "  gsc_data = gsc_data[['clicks','impressions']].reset_index()\n",
        "  #Add a \"false\" day (the first of the month) to our date column to be able to calculate time deltas between two months \n",
        "  gsc_data['date'] = gsc_data['yearMonth']+'01'\n",
        "  #Convert this field to datetime \n",
        "  gsc_data['date'] = pd.to_datetime(gsc_data['date'],format='%Y%m%d')\n",
        "  #Add the URL in our dataFrame\n",
        "  gsc_data['url'] = url\n",
        "  #Add this data to the DF we will be using later on \n",
        "  merged_gsc_data = pd.concat([merged_gsc_data,gsc_data])\n",
        "  i+=1\n",
        "\n",
        "\n",
        "#Function to retrieve the peak month of any given content \n",
        "def get_peak(x):\n",
        "\tdf = merged_gsc_data[merged_gsc_data['url']==x].sort_values(by='clicks',ascending=False)\n",
        "\tpeak = df['yearMonth'].iloc[0]\n",
        "\tpeak = peak+'01'\n",
        "\n",
        "\treturn peak\n",
        "\n",
        "#Function to retrieve the peak clicks of any given content \n",
        "def get_max(x):\n",
        "\tdf = merged_gsc_data[merged_gsc_data['url']==x].sort_values(by='clicks',ascending=False)\n",
        "\tmax_value = df['clicks'].max()\n",
        "\n",
        "\treturn max_value\n",
        "\n",
        "\n",
        "#Retrieve the peak in our merged DF\n",
        "merged_gsc_data['peak'] = merged_gsc_data['url'].apply(get_peak)\n",
        "#Transform the peak to a date object\n",
        "merged_gsc_data['peak'] = pd.to_datetime(merged_gsc_data['peak'],format='%Y%m%d')\n",
        "#Retrieve the max in our merged DF\n",
        "merged_gsc_data['max'] = merged_gsc_data['url'].apply(get_max)\n",
        "#Calculate the loss % between any given month and the peak \n",
        "merged_gsc_data['%_lost'] = ((1-(merged_gsc_data['clicks']/merged_gsc_data['max']))*100).astype(int)\n",
        "#Calculate net loss between any given month and the peak\n",
        "merged_gsc_data['lost'] = merged_gsc_data['clicks'] - merged_gsc_data['max']\n",
        "#Calculate the number of months since the peak (not used but you may need this info)\n",
        "merged_gsc_data['n_since_peak'] = (merged_gsc_data['date'].dt.year - merged_gsc_data['peak'].dt.year)*12 + (merged_gsc_data['date'].dt.month - merged_gsc_data['peak'].dt.month)\n",
        "\n",
        "#Funtion to get the current loss vs peak \n",
        "def get_current_loss_vs_peak(x):\n",
        "  df = merged_gsc_data[merged_gsc_data['url']==x]\n",
        "  value = df[df['date']==df['peak']]['clicks'].sum()-df[df['date']==df['date'].max()]['clicks'].sum()\n",
        "  return value\n",
        "\n",
        "#Functon to get the current % loss vs peak \n",
        "def get_current_loss_percent_vs_peak(x):\n",
        "  df = merged_gsc_data[merged_gsc_data['url']==x]\n",
        "  value = (df[df['date']==df['date'].max()]['clicks'].sum() - df[df['date']==df['peak']]['clicks'].sum())*100/(df[df['date']==df['peak']]['clicks'].sum())\n",
        "\n",
        "  return value\n",
        "\n",
        "\n",
        "# Apply both function to create new columns\n",
        "merged_gsc_data['current_percent_loss'] = merged_gsc_data['url'].apply(get_current_loss_percent_vs_peak)\n",
        "merged_gsc_data['current_loss'] = merged_gsc_data['url'].apply(get_current_loss_vs_peak)\n",
        "\n",
        "# Transform data to get our main decay summary data\n",
        "decay_raw = merged_gsc_data.groupby('url').aggregate({'n_since_peak':np.max,'current_loss':np.max,'current_percent_loss':np.max})\n",
        "decay_raw = decay_raw.sort_values(by='current_loss',ascending=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w0CGfYPnV4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Print first rows to be sure that data are accurate \n",
        "decay.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFWq1DDkniA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create an empty DataFrame for our final result \n",
        "final_data = pd.DataFrame()\n",
        "\n",
        "#Loop urls in our decay_raw DF\n",
        "for url in decay.index.to_list()[0:2]:\n",
        "  #Get the peak date + 1 month as search console library will need the end date and the number of days backwards \n",
        "  #Retrieve peak month using our already used function\n",
        "  peak = get_peak(url)\n",
        "  #Convert it to datetime\n",
        "  peak = datetime.strptime(peak,\"%Y%m%d\")\n",
        "  #Retrieve the number of months in this particular month thanks to the calendar library\n",
        "  days_in_month = calendar.monthrange(peak.year, peak.month)[1]\n",
        "  #Add this number of days \n",
        "  date_for_sc = peak + timedelta(days=days_in_month)\n",
        "  #Convert to string \n",
        "  date_for_sc = date_for_sc.strftime(\"%Y-%m-%d\")\n",
        "  #Retrieve metrics per query for the peak month and the last month\n",
        "  data_peak = account[selected_property].query.filter('page',url,'equals').range(date_for_sc,days=-days_in_month).dimension('query').limit(100).get().to_dataframe()\n",
        "  data_current = account[selected_property].query.filter('page',url,'equals').range('2020-04-30',days=-30).dimension('query').limit(100).get().to_dataframe()\n",
        "  \n",
        "  #Merge both DataFrames on the query column \n",
        "  append = data_peak.merge(data_current,on='query',how='outer',suffixes=('_peak','_current'))\n",
        "  #Add the URL to the merged dataframe\n",
        "  append['url'] = url\n",
        "  #Add data to our final_data DataFrame that we created earlier \n",
        "  final_data = pd.concat([final_data,append])\n",
        "\n",
        "#Check first rows\n",
        "final_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOU0HNCM6Bp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to use the cumsum() \n",
        "def get_cum_sum(x):\n",
        "  df = final_data[final_data['url']==x]\n",
        "  value = df['clicks_diff']*100/df['clicks_diff'].sum()\n",
        "  return value "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K2HZVYbt4Ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Replace NaN caused by our merge by 0. Otherwise, we won't be able to visualize lost keywords for instance. \n",
        "final_data = final_data.fillna(0)\n",
        "#Calculate clicks diff between peak month and current month\n",
        "final_data['clicks_diff'] = final_data['clicks_peak'] - final_data['clicks_current']\n",
        "#Order DF by click diff \n",
        "final_data = final_data.sort_values(by='clicks_diff',ascending=False)\n",
        "#Remove useless columns \n",
        "final_data = final_data[['url','query','clicks_peak','clicks_current','clicks_diff',]]\n",
        "#Remove any keyword with a positive difference, as twe are not interested by these ones here \n",
        "final_data = final_data[final_data['clicks_diff']>0]\n",
        "#Calculate the % of the total diff per keyword \n",
        "final_data['%_diff'] = (final_data['clicks_diff']*100/final_data['clicks_diff'].sum())\n",
        "#Create a cumsum() column to visualize how to apply Pareto principles to our results \n",
        "final_data['cum_%_diff'] = final_data['%_diff'].cumsum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO-jvw5T6gFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Final DF. You can save it in .csv (pd.to_csv()) or send it to Sheets using gspread library for instance. \n",
        "final_data"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}